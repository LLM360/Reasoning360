_wandb:
    value:
        cli_version: 0.21.0
        e:
            6278plci0p355e9dh2wy2uvhjwjtd4ny:
                args:
                    - --node-ip-address=10.24.1.146
                    - --node-manager-port=44053
                    - --object-store-name=/tmp/ray/session_2025-08-05_07-27-40_091244_1999817/sockets/plasma_store
                    - --raylet-name=/tmp/ray/session_2025-08-05_07-27-40_091244_1999817/sockets/raylet
                    - --redis-address=None
                    - --metrics-agent-port=60779
                    - --logging-rotate-bytes=536870912
                    - --logging-rotate-backup-count=5
                    - --runtime-env-agent-port=52665
                    - --gcs-address=10.24.1.146:6379
                    - --session-name=session_2025-08-05_07-27-40_091244_1999817
                    - --temp-dir=/tmp/ray
                    - --webui=127.0.0.1:8265
                    - --cluster-id=21d3be7c933c46f1a6d2d4c21632f83b77ea63893a4bd0e1b4134673
                    - --startup-token=10
                    - --worker-launch-time-ms=1754378894390
                    - --node-id=c765efab0cb0277a3d7544218e48adf86c3161974dc5f0be1cc363b1
                    - --runtime-env-hash=-1053485826
                    - --enable-resource-isolation=false
                cpu_count: 64
                cpu_count_logical: 128
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "1257937666048"
                        used: "81979174912"
                email: liqun1105@gmail.com
                executable: /mnt/weka/home/liqun.ma/miniconda3/envs/Reasoning360/bin/python3
                git:
                    commit: bf5c3dc717f54b8cb8fc0891cf7772c82c36dcad
                    remote: git@github.com:LLM360/Reasoning360.git
                gpu: NVIDIA H200
                gpu_count: 8
                gpu_nvidia:
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-746d75d5-d479-d833-1db1-7ac89557766b
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-a22195c0-8f1e-88f3-8194-13c4676a790e
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-122353cd-1ab0-bcf3-2aba-2bc9093d67e4
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-1ccf5db6-b25a-9242-f942-5f9d3018672a
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-ab0e231f-5006-1c43-b332-da5b7ba47a97
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-51fef2f6-cdd5-c5ef-19fa-55fc1779a565
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-53be4ee6-0667-0eb0-875e-db818c56d36a
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-340918ad-6a41-2288-cc10-5eaac769a43d
                host: fs-mbz-gpu-967
                memory:
                    total: "1690983751680"
                os: Linux-5.15.0-141-generic-x86_64-with-glibc2.35
                program: /mnt/weka/home/liqun.ma/miniconda3/envs/Reasoning360/lib/python3.12/site-packages/ray/_private/workers/default_worker.py
                python: CPython 3.12.11
                root: /mnt/weka/home/liqun.ma/Reasoning360
                slurm:
                    cluster_name: mbzuai
                    conf: /var/spool/slurmd/conf-cache/slurm.conf
                    cpu_bind: quiet,mask_cpu:0x000000000000003F000000000000000F
                    cpu_bind_list: 0x000000000000003F000000000000000F
                    cpu_bind_type: 'mask_cpu:'
                    cpu_bind_verbose: quiet
                    cpus_on_node: "10"
                    cpus_per_task: "10"
                    distribution: cyclic
                    gpus_on_node: "8"
                    gtids: "0"
                    job_account: iq
                    job_cpus_per_node: "10"
                    job_end_time: "1785914825"
                    job_gid: "4556"
                    job_gpus: 0,1,2,3,4,5,6,7
                    job_id: "561181"
                    job_name: test_rl
                    job_nodelist: fs-mbz-gpu-967
                    job_num_nodes: "1"
                    job_partition: main
                    job_qos: iq
                    job_start_time: "1754378825"
                    job_uid: "4556"
                    job_user: liqun.ma
                    jobid: "561181"
                    launch_node_ipaddr: 10.24.1.146
                    localid: "0"
                    mem_per_node: "524288"
                    mpi_type: pmi2
                    nnodes: "1"
                    nodeid: "0"
                    nodelist: fs-mbz-gpu-967
                    nprocs: "1"
                    ntasks: "1"
                    ntasks_per_node: "1"
                    prio_process: "0"
                    procid: "0"
                    srun_comm_host: 10.24.1.146
                    srun_comm_port: "39531"
                    step_gpus: 0,1,2,3,4,5,6,7
                    step_id: "3"
                    step_launcher_port: "39531"
                    step_nodelist: fs-mbz-gpu-967
                    step_num_nodes: "1"
                    step_num_tasks: "1"
                    step_tasks_per_node: "1"
                    stepid: "3"
                    submit_dir: /mnt/weka/home/liqun.ma/Reasoning360
                    submit_host: fs-mbz-login-001
                    task_pid: "1999817"
                    tasks_per_node: "1"
                    topology_addr: root.POD2.POD2.SU1.fs-mbz-gpu-967
                    topology_addr_pattern: switch.switch.switch.node
                    tres_per_task: cpu=10
                    umask: "0022"
                startedAt: "2025-08-05T07:31:37.737377Z"
                writerId: 6278plci0p355e9dh2wy2uvhjwjtd4ny
        m: []
        python_version: 3.12.11
        t:
            "1":
                - 1
                - 11
                - 30
                - 41
                - 49
                - 50
                - 71
                - 95
                - 98
                - 105
            "2":
                - 1
                - 11
                - 30
                - 41
                - 49
                - 50
                - 71
                - 95
                - 98
                - 105
            "3":
                - 2
                - 13
                - 16
            "4": 3.12.11
            "5": 0.21.0
            "6": 4.54.1
            "12": 0.21.0
            "13": linux-x86_64
actor_rollout_ref:
    value:
        actor:
            checkpoint:
                contents:
                    - model
                    - hf_model
                    - optimizer
                    - extra
            clip_ratio: 0.2
            clip_ratio_c: 10
            clip_ratio_high: 0.2
            clip_ratio_low: 0.2
            entropy_coeff: 0
            fsdp_config:
                fsdp_size: -1
                optimizer_offload: true
                param_offload: true
                wrap_policy:
                    min_num_params: 0
            grad_clip: 1
            kl_loss_coef: 0
            kl_loss_type: low_var_kl
            loss_agg_mode: token-mean
            optim:
                lr: 1e-06
                lr_warmup_steps: 10
                lr_warmup_steps_ratio: 0
                min_lr_ratio: 0
                total_training_steps: 15
                warmup_style: constant
                weight_decay: 0.1
            ppo_epochs: 1
            ppo_max_token_len_per_gpu: 24576
            ppo_micro_batch_size: null
            ppo_micro_batch_size_per_gpu: null
            ppo_mini_batch_size: 64
            shuffle: false
            strategy: fsdp
            ulysses_sequence_parallel_size: 4
            use_dynamic_bsz: true
            use_kl_loss: false
            use_torch_compile: true
        hybrid_engine: true
        model:
            enable_gradient_checkpointing: true
            external_lib: null
            override_config:
                attention_dropout: 0
                embd_pdrop: 0
                resid_pdrop: 0
            path: Qwen/Qwen2.5-7B-instruct
            use_remove_padding: true
        ref:
            fsdp_config:
                param_offload: true
                wrap_policy:
                    min_num_params: 0
            log_prob_max_token_len_per_gpu: 24576
            log_prob_micro_batch_size: null
            log_prob_micro_batch_size_per_gpu: null
            log_prob_use_dynamic_bsz: true
            ulysses_sequence_parallel_size: 4
        rollout:
            disable_log_stats: true
            do_sample: true
            dtype: bfloat16
            enable_chunked_prefill: true
            enforce_eager: true
            free_cache_engine: true
            gpu_memory_utilization: 0.7
            ignore_eos: false
            load_format: dummy_dtensor
            log_prob_max_token_len_per_gpu: 24576
            log_prob_micro_batch_size: null
            log_prob_micro_batch_size_per_gpu: null
            log_prob_use_dynamic_bsz: true
            max_model_len: null
            max_num_batched_tokens: 24576
            max_num_seqs: 1024
            mode: sync
            multi_turn:
                enable: false
            "n": 16
            name: vllm
            prompt_length: 4096
            response_length: 8192
            temperature: 1
            tensor_model_parallel_size: 4
            top_k: -1
            top_p: 1
            use_fire_sampling: false
            val_kwargs:
                do_sample: true
                "n": 1
                temperature: 1
                top_k: -1
                top_p: 1
algorithm:
    value:
        adv_estimator: grpo
        filter_groups:
            enable: false
            max_num_gen_batches: 10
            metric: acc
        gamma: 1
        kl_ctrl:
            horizon: 10000
            kl_coef: 0
            target_kl: 0.1
            type: fixed
        kl_penalty: kl
        lam: 1
        use_kl_in_reward: false
critic:
    value:
        checkpoint:
            contents:
                - model
                - hf_model
                - optimizer
                - extra
        cliprange_value: 0.5
        forward_max_token_len_per_gpu: 32768
        forward_micro_batch_size: null
        forward_micro_batch_size_per_gpu: null
        grad_clip: 1
        model:
            enable_gradient_checkpointing: true
            external_lib: null
            fsdp_config:
                fsdp_size: -1
                optimizer_offload: false
                param_offload: false
                wrap_policy:
                    min_num_params: 0
            path: ~/models/deepseek-llm-7b-chat
            tokenizer_path: Qwen/Qwen2.5-7B-instruct
            use_remove_padding: false
        optim:
            lr: 1e-05
            lr_warmup_steps_ratio: 0
            min_lr_ratio: null
            total_training_steps: 15
            warmup_style: constant
            weight_decay: 0.01
        ppo_epochs: 1
        ppo_max_token_len_per_gpu: 32768
        ppo_micro_batch_size: null
        ppo_micro_batch_size_per_gpu: null
        ppo_mini_batch_size: 64
        rollout_n: 16
        shuffle: false
        strategy: fsdp
        ulysses_sequence_parallel_size: 1
        use_dynamic_bsz: true
custom_reward_function:
    value:
        name: compute_score
        path: null
data:
    value:
        filter_overlong_prompts: false
        gen_batch_size: 512
        image_key: images
        max_prompt_length: 4096
        max_response_length: 8192
        prompt_key: prompt
        return_full_prompt: false
        return_raw_chat: false
        return_raw_input_ids: false
        reward_fn_key: data_source
        shuffle: true
        tokenizer: null
        train_batch_size: 512
        train_files:
            - ./data/train/synlogic.parquet
        truncation: right
        trust_remote_code: true
        val_batch_size: null
        val_files:
            - ./data/train/synlogic.parquet
ray_init:
    value:
        num_cpus: null
reward_model:
    value:
        enable: false
        forward_max_token_len_per_gpu: 32768
        launch_reward_fn_async: false
        max_length: null
        micro_batch_size: null
        micro_batch_size_per_gpu: null
        model:
            external_lib: null
            fsdp_config:
                fsdp_size: -1
                param_offload: false
                wrap_policy:
                    min_num_params: 0
            input_tokenizer: Qwen/Qwen2.5-7B-instruct
            path: ~/models/FsfairX-LLaMA3-RM-v0.1
            use_remove_padding: false
        overlong_buffer:
            enable: false
            len: 4096
            log: true
            penalty_factor: 1
        reward_manager: async_dapo
        strategy: fsdp
        ulysses_sequence_parallel_size: 1
        use_dynamic_bsz: true
trainer:
    value:
        balance_batch: true
        critic_warmup: 0
        default_hdfs_dir: null
        default_local_dir: checkpoints/Reasoning360/561181-test_rl-Qwen2.5-7B-instruct
        del_local_ckpt_after_load: false
        experiment_name: 561181-test_rl-Qwen2.5-7B-instruct
        log_val_generations: 0
        logger:
            - console
            - wandb
        max_actor_ckpt_to_keep: null
        max_critic_ckpt_to_keep: null
        n_gpus_per_node: 8
        nnodes: 1
        project_name: Reasoning360
        remove_previous_ckpt_in_save: false
        resume_from_path: null
        resume_mode: auto
        save_freq: 5
        test_freq: 5
        total_epochs: 5
        total_training_steps: null
        val_before_train: true
        val_generations_to_log_to_wandb: 30
