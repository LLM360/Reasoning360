hydra:
  searchpath:
    - file://verl/trainer/config # First Loads the base configuration from verl/trainer/config/ppo_trainer.yaml which contains all the default hyperparameters. Then loads and applies recipe/dapo/src/config/dapo_trainer.yaml which can override or extend the base configuration.

defaults:
  - ppo_trainer
  - _self_

data:
  filter_overlong_prompts: False
  truncation: error
  image_key: images
  trust_remote_code: True


reward_model:
  reward_manager: dapo
  overlong_buffer: 
    enable: False # We try to avoid forgetting to set enable
    len: 0
    penalty_factor: 0.0
    log: True

custom_reward_function:
  path: null
  name: compute_score

algorithm:
  filter_groups:
    enable: False # We try to avoid forgetting to set enable
    metric: null # acc / score / seq_reward / seq_final_reward / ...
    max_num_gen_batches: 0 # Non-positive values mean no upper limit

trainer:
  balance_batch: True
  total_epochs: 30
  total_training_steps: null
  project_name: verl_examples
  experiment_name: gsm8k
  logger: [ 'console', 'wandb' ]
  log_val_generations: 0
  nnodes: 1
  n_gpus_per_node: 8
  save_freq: -1
  # auto: find the last ckpt to resume. If can't find, start from scratch
  resume_mode: auto # or disable or resume_path if resume_from_path is set
  resume_from_path: null
  val_before_train: True
  test_freq: -1
  critic_warmup: 0
  default_hdfs_dir: null
  remove_previous_ckpt_in_save: False
  del_local_ckpt_after_load: False
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
  max_actor_ckpt_to_keep: null
  max_critic_ckpt_to_keep: null

curriculum:
  enable: True
  
actor_rollout_ref:
  rollout:
    n: 4
    n_continue: 12